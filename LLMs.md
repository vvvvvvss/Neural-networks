# Large Language Models and Transformers
Large Language Model, abbreviated as LLMs, is a type of AI program that uses deep learning and large amounts of data to understand, generate, and predict new content. LLMs are built on machine learning, specifically a type of neural network called a transformer model. They can recognize and generate text, and can be used for a number of tasks including writing code, summarizing, translation.

The underlying transformer is a set of neural networks that consist of an encoder and a decoder with self-attention capabilities. The encoder and decoder extract meanings from a sequence of text and understand the relationships between words and phrases in it.

A key factor in how LLMs work is the way they represent words. Earlier forms of machine learning used a numerical table to represent each word. But, this form of representation could not recognize relationships between words such as words with similar meanings. This limitation was overcome by using multi-dimensional vectors, commonly referred to as word embeddings, to represent words so that words with similar contextual meanings or other relationships are close to each other in the vector space.

## Tokenization
Tokenization is the process of converting a sequence of text into smaller parts, known as tokens.
![image](https://github.com/vvvvvvss/Neural-networks/assets/148562671/b57c3229-7d7c-41e7-8fb3-42b424621147)
## GPT- 4
GPT stands for Generative Pre-Trained Transformer, a flagship model released by OpenAI in 2018. It is a language model developed to get text as if it were generated by humans. It has outperformed several other AI language models including Google’s BERT.

GPT has several layers of transformers stacked over each other. Each layer takes input from the previous layer, processes it using self-attention and feed-forward layers, and then passes its output to the next layer in the architecture. The output from the final layer is used to get the predicted text.

## Building GPT-4
### Step 1:

In order to create an effective chatbot, you need to know the use cases. This will help you create a plan and build it accordingly. You have to identify the target audience and the purpose of the chatbot.

### Step 2:

For this chatbot, we will be using Python. However, you can use other programming languages like Ruby, and Node.js.

### Step 3:

Set up a virtual environment by installing the following Python library through the command line:

`pip install virtualenv`
Enter the project folder and type the command:

`virtualenv chatbot_venv`
The above command creates a virtual environment named “chatbot_venv”.

To activate the virtual environment, type the command:

`chatbot_venv\scripts\activate`
The virtual environment will get activated which will allow you to get the list of dependencies through a single command mentioned below:

`Pip freeze> requirements.txt`
### Step 4:

Install the OpenAI library using this command in the virtual environment:

`pip install openai`
### Step 5:

In this step, you need to create an environment file with a .env extension for storing all the environment variables. You store the OpenAI API key with:

`API_KEY=<your_api_key>`
You will need to use python-dotenv to access the API key:

#### importing all the relevant libraries
`import openai
from dotenv import load_dotenv
load_dotenv()
api_key_openai = os.environ.get("API_KEY")
openai.api_key = api_key_openai`

### Step 7:

In this step, we will write a function that interacts with the GPT-4 API. It accepts the following parameters:

Temperature: A measure of the randomness of a text generated by the model. Its value lies between 0 and 1. Values closer to 1 mean that the output is more random, whereas values close to 0 mean the output is very relatable.
Top_p: An alternative to the above method to add randomness to the output.
N: Signifies the number of responses generated by the API and provides a choice to the developer to select the best response.
Max_tokens: Signifies the length of a response generated by the API, i.e., the number of words present in the text.
`def get_response(prompt, model="text-curie-002", max_tokens=150):
    response = openai.Completion.create(
        engine=model, #or "your_chosen_engine",
        prompt=prompt,
        max_tokens=max_tokens,
        n=5,
        stop=None,
        temperature=0.8,
    )  return response.choices[0].text.strip()`
We have seen how to call an API to get a response from the GPT-4 model. Now, we will customize our own dataset for a customized chatbot.
